# IntegraciÃ³n Spark + MongoDB para 10M Usuarios - EU-UTVT

## ğŸ¯ Sistema de Big Data Analytics

Este sistema integra **Apache Spark** con **MongoDB** para realizar anÃ¡lisis masivos de 10 millones de usuarios del sistema EU-UTVT.

## ğŸ“ Archivos del Sistema

### ğŸ”¬ `spark_mongo_analytics.py` - AnÃ¡lisis Principal
**Motor de anÃ¡lisis principal con Spark + MongoDB**

**CaracterÃ­sticas:**
- âœ… ConexiÃ³n directa a MongoDB con 10M usuarios
- âœ… Carga optimizada de datos en lotes de 50K
- âœ… DataFrame Spark con schema optimizado
- âœ… AnÃ¡lisis bÃ¡sico y avanzado con Spark SQL
- âœ… Pruebas de rendimiento masivo
- âœ… ExportaciÃ³n de resultados

**Funcionalidades:**
1. **AnÃ¡lisis BÃ¡sico**: EstadÃ­sticas generales, distribuciÃ³n por roles/departamentos
2. **AnÃ¡lisis Avanzado**: Consultas SQL complejas, anÃ¡lisis temporal, rankings
3. **Pruebas de Rendimiento**: Filtrado masivo, agregaciones, joins
4. **ExportaciÃ³n**: Resultados en CSV para anÃ¡lisis posterior

### ğŸ“Š `spark_reports_generator.py` - Generador de Reportes
**Sistema automatizado de reportes empresariales**

**CaracterÃ­sticas:**
- âœ… Reportes ejecutivos automatizados
- âœ… AnÃ¡lisis departamental detallado
- âœ… Tendencias de actividad y retenciÃ³n
- âœ… MÃ©tricas de rendimiento del sistema
- âœ… ExportaciÃ³n en mÃºltiples formatos

**Tipos de Reportes:**
1. **Ejecutivo**: Resumen de alto nivel para directivos
2. **Departamental**: AnÃ¡lisis detallado por cada departamento
3. **Tendencias**: Actividad temporal y patrones de uso
4. **Rendimiento**: MÃ©tricas del sistema y optimizaciones

## ğŸš€ ConfiguraciÃ³n del Entorno

### âœ… Tu configuraciÃ³n actual:
- **Spark 4.0.1** instalado en `C:\spark`
- **PySpark 4.0.1** en entorno `spark_env`
- **MongoDB Server** ejecutÃ¡ndose
- **PyMongo + Motor** para conectividad
- **Pandas + NumPy** para anÃ¡lisis complementario

### ğŸ”§ Variables de entorno configuradas:
```bash
SPARK_HOME = C:\spark
PYSPARK_PYTHON = python (del entorno spark_env)
```

## ğŸ“Š Capacidades del Sistema

### ğŸ¯ **Rendimiento Esperado:**

| OperaciÃ³n | 100K usuarios | 1M usuarios | 10M usuarios |
|-----------|---------------|-------------|--------------|
| Carga de datos | ~3 segundos | ~30 segundos | ~5 minutos |
| Filtrado simple | ~0.1 segundos | ~0.5 segundos | ~3 segundos |
| AgregaciÃ³n compleja | ~0.5 segundos | ~2 segundos | ~15 segundos |
| Join masivo | ~1 segundo | ~5 segundos | ~30 segundos |

### ğŸ“ˆ **Tipos de AnÃ¡lisis Disponibles:**

#### ğŸ” **AnÃ¡lisis BÃ¡sico:**
- Total de usuarios y tasa de actividad
- DistribuciÃ³n por roles (solicitante, aprobador, pagador, admin)
- DistribuciÃ³n por departamentos
- EstadÃ­sticas de login y actividad

#### ğŸ§  **AnÃ¡lisis Avanzado:**
- AnÃ¡lisis temporal de registros
- Actividad por departamento y rol cruzado
- Rankings de usuarios mÃ¡s activos
- AnÃ¡lisis de dominios de email
- Patrones de uso por dÃ­a de la semana

#### ğŸ“Š **AnÃ¡lisis de Tendencias:**
- Crecimiento de usuarios por mes
- RetenciÃ³n de usuarios (Ãºltima semana, mes, aÃ±o)
- Actividad por dÃ­a de la semana
- AnÃ¡lisis de cohortes

#### ğŸš€ **AnÃ¡lisis de Rendimiento:**
- Benchmarks de consultas
- MÃ©tricas de throughput
- Optimizaciones de cluster
- Recomendaciones de escalabilidad

## ğŸ® Uso del Sistema

### ğŸš€ **OpciÃ³n 1: AnÃ¡lisis Completo (Recomendado)**
```bash
# Activar entorno Spark
conda activate spark_env

# Ejecutar anÃ¡lisis principal
python spark_mongo_analytics.py
```

**MenÃº disponible:**
1. ğŸ“ˆ AnÃ¡lisis bÃ¡sico
2. ğŸ”¬ AnÃ¡lisis avanzado (Spark SQL)
3. ğŸš€ Pruebas de rendimiento
4. ğŸ’¾ Exportar resultados
5. ğŸ”„ Recargar datos

### ğŸ“‹ **OpciÃ³n 2: GeneraciÃ³n de Reportes**
```bash
# Generar reportes empresariales
python spark_reports_generator.py
```

**Reportes disponibles:**
1. ğŸ“Š Reporte ejecutivo
2. ğŸ¢ Reportes por departamento
3. ğŸ“ˆ AnÃ¡lisis de tendencias
4. ğŸš€ Reporte de rendimiento
5. ğŸ“‘ Generar todos los reportes

## ğŸ“ Estructura de Salida

### ğŸ“Š **AnÃ¡lisis Principal:**
```
spark_analysis_results/
â”œâ”€â”€ role_distribution/          # DistribuciÃ³n por roles
â”œâ”€â”€ department_distribution/    # DistribuciÃ³n por departamentos
â””â”€â”€ detailed_statistics/        # EstadÃ­sticas detalladas
```

### ğŸ“‹ **Reportes:**
```
reportes_departamentos_YYYYMMDD_HHMMSS/
â”œâ”€â”€ reporte_RectorÃ­a.txt
â”œâ”€â”€ reporte_Sistemas_y_TI.txt
â”œâ”€â”€ reporte_Finanzas.txt
â””â”€â”€ ...

reporte_ejecutivo_YYYYMMDD_HHMMSS.txt
analisis_tendencias_YYYYMMDD_HHMMSS.txt
reporte_rendimiento_YYYYMMDD_HHMMSS.txt
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### âš¡ **OptimizaciÃ³n para 10M usuarios:**

**ConfiguraciÃ³n Spark en los scripts:**
```python
.config("spark.sql.adaptive.enabled", "true")
.config("spark.sql.adaptive.coalescePartitions.enabled", "true")
.config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
.config("spark.sql.execution.arrow.pyspark.enabled", "true")
.config("spark.driver.memory", "6g")
.config("spark.executor.memory", "4g")
```

**MongoDB optimizado:**
- Ãndices en campos crÃ­ticos (email, role, department)
- Consultas por lotes de 50K registros
- ProyecciÃ³n de campos especÃ­ficos

### ğŸ›ï¸ **PersonalizaciÃ³n:**

**Modificar tamaÃ±os de lote:**
```python
# En spark_mongo_analytics.py lÃ­nea 75
batch_size = 50000  # Cambiar segÃºn RAM disponible

# En los scripts de generaciÃ³n
BATCH_SIZE = 1000   # Para generaciÃ³n de usuarios
```

**Ajustar memoria Spark:**
```python
# Aumentar si tienes mÃ¡s RAM
.config("spark.driver.memory", "8g")
.config("spark.executor.memory", "6g")
```

## ğŸ“ˆ Consultas SQL Ejemplo

### ğŸ” **Top usuarios por departamento:**
```sql
SELECT department, first_name, last_name, login_count,
       ROW_NUMBER() OVER (PARTITION BY department ORDER BY login_count DESC) as rank
FROM users 
WHERE login_count > 0
```

### ğŸ“Š **AnÃ¡lisis de actividad mensual:**
```sql
SELECT DATE_FORMAT(created_at, 'yyyy-MM') as month,
       COUNT(*) as new_users,
       SUM(COUNT(*)) OVER (ORDER BY DATE_FORMAT(created_at, 'yyyy-MM')) as cumulative
FROM users 
GROUP BY month
ORDER BY month
```

### ğŸ¢ **DistribuciÃ³n departamento-rol:**
```sql
SELECT department, role, 
       COUNT(*) as total_users,
       AVG(login_count) as avg_activity,
       SUM(CASE WHEN is_active THEN 1 ELSE 0 END) as active_users
FROM users 
GROUP BY department, role
ORDER BY total_users DESC
```

## ğŸ› ï¸ Troubleshooting

### âŒ **Error: Java no encontrado**
```bash
# Instalar Java 8 o 11
# Verificar JAVA_HOME
echo $env:JAVA_HOME
```

### âŒ **Error: Memoria insuficiente**
```python
# Reducir configuraciÃ³n de memoria
.config("spark.driver.memory", "2g")
.config("spark.executor.memory", "2g")
```

### âŒ **Error: MongoDB connection**
```bash
# Verificar servicio MongoDB
Get-Service | Where-Object {$_.Name -like "*mongo*"}
```

### âš ï¸ **Performance lento:**
1. Aumentar `spark.default.parallelism`
2. Usar mÃ¡s particiones: `df.repartition(200)`
3. Cachear DataFrames frecuentes: `df.cache()`

## ğŸ¯ Casos de Uso Empresariales

### ğŸ‘¨â€ğŸ’¼ **Para Directivos:**
- Reportes ejecutivos automatizados
- MÃ©tricas de adopciÃ³n del sistema
- AnÃ¡lisis de departamentos mÃ¡s activos
- ROI del sistema de solicitudes

### ğŸ‘©â€ğŸ’» **Para Administradores TI:**
- MÃ©tricas de rendimiento del sistema
- AnÃ¡lisis de carga por departamento
- IdentificaciÃ³n de usuarios inactivos
- OptimizaciÃ³n de recursos

### ğŸ“Š **Para Analistas:**
- AnÃ¡lisis de tendencias de uso
- SegmentaciÃ³n de usuarios
- AnÃ¡lisis de retenciÃ³n
- PredicciÃ³n de crecimiento

## ğŸš€ Escalabilidad

**El sistema estÃ¡ diseÃ±ado para escalar:**
- âœ… **10M usuarios actuales**
- âœ… **50M usuarios** (con mÃ¡s RAM)
- âœ… **100M usuarios** (cluster distribuido)
- âœ… **IntegraciÃ³n con Hadoop/HDFS**
- âœ… **Streaming analytics** con Spark Streaming

## ğŸ“ Soporte

Para optimizaciones especÃ­ficas o escalamiento del cluster:
1. Ajustar configuraciÃ³n de memoria segÃºn hardware
2. Configurar cluster Spark distribuido para datasets > 50M
3. Implementar particionamiento por fecha/departamento
4. Integrar con herramientas de visualizaciÃ³n (Tableau, Power BI)

Â¡El sistema estÃ¡ listo para analizar 10 millones de usuarios con Apache Spark! ğŸš€